import httpx
import json
import asyncio
import logging
import os
from dataclasses import dataclass
from enum import Enum
from typing import Dict, List, Any, Optional

import yaml
from opentelemetry import trace

from cherry.core.agent_orchestrator import AgentOrchestrator
from cherry.llm.deepseek_client import DeepSeekClient  # Assumes you have implemented this client

# Configure logging
logger = logging.getLogger("cherry.coding")

# Configure OpenTelemetry
tracer = trace.get_tracer("cherry.coding.agents")


class CodeReviewStage(Enum):
    """Stages of the code review process"""
    INITIAL_GENERATION = "initial_generation"
    CROSS_VALIDATION = "cross_validation"
    DYNAMIC_TESTING = "dynamic_testing"
    HUMAN_LIKE_REVIEW = "human_like_review"
    SECURITY_REVIEW = "security_review"
    PERFORMANCE_REVIEW = "performance_review"


@dataclass
class CodeContext:
    """Context information for code generation and review"""
    project_name: str
    file_path: str
    language: str
    requirements: str
    # Mapping file path → content
    context_files: Optional[Dict[str, str]] = None
    dependencies: Optional[Dict[str, str]] = None     # Package name → version
    # Test file path → content
    test_files: Optional[Dict[str, str]] = None


@dataclass
class CodeSolution:
    """A code solution generated by an agent"""
    agent_name: str
    code: str
    explanation: str
    confidence: float
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class ValidationResult:
    """Result of code validation"""
    stage: CodeReviewStage
    agent_name: str
    is_valid: bool
    score: float  # normalized between 0 and 1
    issues: List[Dict[str, Any]]
    suggestions: List[Dict[str, Any]]
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class TestResult:
    """Result of code testing"""
    passed: bool
    total_tests: int
    passed_tests: int
    test_details: List[Dict[str, Any]]
    performance_metrics: Dict[str, Any]
    metadata: Optional[Dict[str, Any]] = None


class CherryCodeOrchestrator:
    """
    Specialized orchestrator for coding agents that integrates DeepSeek-V3 for code
    generation, review, and improvement. It provides a full cycle to generate a solution,
    validate it, and iteratively improve it until the quality threshold is met.
    """
    
    def __init__(self, api_key: str = None):
    def __init__(self, api_key: str = None):
        self.orchestrator = AgentOrchestrator()
        self.deepseek_client = DeepSeekClient(api_key)
        # file_path → CodeSolution
        self.solutions: Dict[str, CodeSolution] = {}
        # file_path → List[ValidationResult]
        self.validation_results: Dict[str, List[ValidationResult]] = {}

    async def generate_solution(self, context: CodeContext) -> CodeSolution:
        """Generate a code solution using DeepSeek-V3."""
        with tracer.start_as_current_span("generate_solution") as span:
            span.set_attribute("file_path", context.file_path)
            logger.info(f"Generating solution for {context.file_path}")
            solution = await self.deepseek_client.generate_code(context)
            self.solutions[context.file_path] = solution
            num_lines = len(solution.code.splitlines())
            logger.info(
                f"Generated solution for {context.file_path} ({num_lines} lines)")
            span.set_attribute("code_lines", num_lines)
            return solution

    async def validate_solution(self,
                                file_path: str,
                                context: CodeContext,
                                stage: CodeReviewStage = CodeReviewStage.INITIAL_GENERATION
                                ) -> ValidationResult:
        """Validate a previously generated solution using DeepSeek-V3."""
        with tracer.start_as_current_span("validate_solution") as span:
            span.set_attribute("file_path", file_path)
            span.set_attribute("review_stage", stage.value)

            solution = self.solutions.get(file_path)
            if not solution:
                error_msg = f"No solution found for {file_path}"
                logger.error(error_msg)
                raise ValueError(error_msg)

            logger.info(
                f"Validating solution for {file_path} at stage {stage.value}")
            # Map internal stage to DeepSeek's expected keywords
            stage_map = {
                CodeReviewStage.INITIAL_GENERATION: "general",
                CodeReviewStage.CROSS_VALIDATION: "general",
                CodeReviewStage.SECURITY_REVIEW: "security",
                CodeReviewStage.PERFORMANCE_REVIEW: "performance",
                CodeReviewStage.DYNAMIC_TESTING: "testing",
                CodeReviewStage.HUMAN_LIKE_REVIEW: "general",
            }
            review_stage = stage_map.get(stage, "general")

            validation = await self.deepseek_client.review_code(
                code=solution.code,
                context=context,
                review_stage=review_stage
            )

            if file_path not in self.validation_results:
                self.validation_results[file_path] = []
            self.validation_results[file_path].append(validation)

            logger.info(f"Validation for {file_path}: valid={validation.is_valid}, "
                        f"score={validation.score:.2f}, issues={len(validation.issues)}")
            span.set_attribute("validation_score", validation.score)
            span.set_attribute("issues_count", len(validation.issues))
            return validation

    async def improve_solution(self, file_path: str, context: CodeContext) -> CodeSolution:
        """Improve a solution based on validation feedback using DeepSeek-V3."""
        with tracer.start_as_current_span("improve_solution") as span:
            span.set_attribute("file_path", file_path)
            solution = self.solutions.get(file_path)
            if not solution:
                error_msg = f"No solution found for {file_path}"
                logger.error(error_msg)
                raise ValueError(error_msg)

            validations = self.validation_results.get(file_path, [])
            if not validations:
                logger.warning(
                    f"No validation results found for {file_path}, cannot improve")
                return solution

            # Gather all issues and suggestions from validation results
            all_issues = []
            all_suggestions = []
            for validation in validations:
                all_issues.extend(validation.issues)
                all_suggestions.extend(validation.suggestions)

            logger.info(
                f"Improving solution for {file_path} based on {len(all_issues)} issues and {len(all_suggestions)} suggestions")

            improved_code = await self.deepseek_client.improve_code(
                code=solution.code,
                context=context,
                issues=all_issues,
                suggestions=all_suggestions
            )
            improvement_explanation = (
                f"Code improved to address {len(all_issues)} issues and implement {len(all_suggestions)} suggestions."
            )
            improved_solution = CodeSolution(
                agent_name="deepseek_v3",
                code=improved_code,
                explanation=improvement_explanation,
                confidence=0.95,
                metadata={
                    "model": "deepseek-v3",
                    "improved": True,
                    "original_solution_id": id(solution),
                    "issues_addressed": len(all_issues),
                    "suggestions_implemented": len(all_suggestions)
                }
            )
            self.solutions[file_path] = improved_solution
            logger.info(f"Improved solution for {file_path}")
            span.set_attribute("improved_code_lines",
                               len(improved_code.splitlines()))
            return improved_solution

    async def run_full_cycle(self,
                             context: CodeContext,
                             max_iterations: int = 3,
                             quality_threshold: float = 0.85
                             ) -> CodeSolution:
        """
        Run a full cycle: generate → validate → improve until the quality threshold is reached.

        Args:
            context: CodeContext for code generation
            max_iterations: Maximum number of improvement iterations
            quality_threshold: Score threshold (0-1) for acceptable code

        Returns:
            Final CodeSolution that meets the quality requirements.
        """
        with tracer.start_as_current_span("run_full_cycle") as span:
            span.set_attribute("file_path", context.file_path)
            span.set_attribute("max_iterations", max_iterations)
            span.set_attribute("quality_threshold", quality_threshold)
            logger.info(
                f"Starting full cycle for {context.file_path} (threshold: {quality_threshold}, max iterations: {max_iterations})")

            # Generate the initial solution
            solution = await self.generate_solution(context)
            iterations = 0
            best_score = 0.0

            while iterations < max_iterations:
                validation = await self.validate_solution(
                    context.file_path,
                    context,
                    CodeReviewStage.CROSS_VALIDATION if iterations > 0 else CodeReviewStage.INITIAL_GENERATION
                )
                best_score = max(best_score, validation.score)
                if validation.score >= quality_threshold:
                    logger.info(
                        f"Quality threshold reached for {context.file_path}: score {validation.score:.2f}")
                    break
                if iterations == max_iterations - 1:
                    logger.info(
                        f"Max iterations reached for {context.file_path}")
                    break
                logger.info(
                    f"Iteration {iterations + 1}/{max_iterations}: Current score {validation.score:.2f} is below threshold {quality_threshold}")
                solution = await self.improve_solution(context.file_path, context)
                iterations += 1

            # Final validation if needed
            if best_score < quality_threshold and iterations > 0:
                final_validation = await self.validate_solution(
                    context.file_path,
                    context,
                    CodeReviewStage.CROSS_VALIDATION
                )
                best_score = max(best_score, final_validation.score)

            logger.info(
                f"Full cycle completed for {context.file_path}: iterations={iterations}, final score={best_score:.2f}")
            span.set_attribute("iterations", iterations)
            span.set_attribute("final_score", best_score)
            return self.solutions[context.file_path]
